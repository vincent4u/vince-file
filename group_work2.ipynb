{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyotXlUwOGTgwiqcYLRaNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/vince-file/blob/main/group_work2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh2bNVGewUNU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data1 = pd.read_csv(\"/content/train.csv\")\n",
        "data2 = pd.read_csv(\"/content/train_preprocessed.csv\", compression='gzip')\n",
        "data_sales = data1['Sales']\n",
        "\n",
        "# Concatenate the data\n",
        "rossman_processed_data = pd.concat([data2, data_sales], axis=1)\n",
        "rossman_features = rossman_processed_data.iloc[:, :-1].values\n",
        "rossman_target1 = rossman_processed_data.iloc[:, -1].values\n",
        "\n",
        "# Normalizing the target variable\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rossman_target = scaler.fit_transform(rossman_target1.reshape(-1, 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(rossman_features, rossman_target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data for LSTM input [samples, time steps, features]\n",
        "X_train = X_train.reshape((-1, 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((-1, 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(1, X_train.shape[2])))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', test_loss)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Denormalizing the predicted values\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "y_test = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Convert predictions to binary\n",
        "threshold = 0.5\n",
        "predictions_binary = np.where(predictions >= threshold, 1, 0)\n",
        "y_test_binary = np.where(y_test >= threshold, 1, 0)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test_binary, predictions_binary)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# Plot actual vs. predicted sales\n",
        "plt.plot(y_test, label='Actual')\n",
        "plt.plot(predictions, label='Predicted')\n",
        "plt.xlabel('Data Point')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Actual vs. Predicted Sales')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss values\n",
        "loss_history = history.history['loss']\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-d_pfmmHyHue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an explanation of the code in simple terms:\n",
        "\n",
        "1.First, we import the necessary libraries: pandas, numpy, MinMaxScaler, train_test_split, Sequential, Dense, accuracy_score, and matplotlib.pyplot. These libraries provide various tools and functions for data manipulation, preprocessing, model building, evaluation, and visualization.\n",
        "\n",
        "2.We load the data from two CSV files, \"train.csv\" and \"train_preprocessed.csv\", using pandas. The data contains information about sales.\n",
        "\n",
        "3.We concatenate the two datasets, combining the preprocessed data and the sales column into a single DataFrame called \"rossman_processed_data\".\n",
        "\n",
        "4.We separate the features (rossman_features) and the target variable (rossman_target1) from the concatenated DataFrame.\n",
        "\n",
        "5.The target variable is normalized using the MinMaxScaler, which scales the values between 0 and 1.\n",
        "\n",
        "6.The data is split into training and testing sets using the train_test_split function. 80% of the data is used for training (X_train and y_train), and 20% is used for testing (X_test and y_test).\n",
        "\n",
        "7.The data is reshaped to fit the input requirements of an LSTM model. The shape is changed to (samples, time steps, features) using the reshape function.\n",
        "\n",
        "8.We build an LSTM model using the Sequential class from Keras. The model has an LSTM layer with 128 units and a Dense layer with 1 unit.\n",
        "\n",
        "9.The model is compiled with a mean squared error (MSE) loss function and the Adam optimizer.\n",
        "\n",
        "10.The model is trained on the training data for 10 epochs, with a batch size of 32.\n",
        "\n",
        "11.The model is evaluated on the testing data, and the test loss is calculated and printed.\n",
        "\n",
        "12.Predictions are made on the testing data using the trained model.\n",
        "\n",
        "13.The predicted values are denormalized using the inverse_transform function of the scaler.\n",
        "\n",
        "14.The predicted and actual values are converted to binary values using a threshold of 0.5.\n",
        "\n",
        "15.The accuracy of the predictions is calculated using the accuracy_score function.\n",
        "\n",
        "16.The actual and predicted sales values are plotted using matplotlib.\n",
        "\n",
        "17.The loss values during training are plotted to visualize the training progress.\n",
        "18. Accuracy: 0.8630912004404203\n",
        "I hope this explanation helps! Let me know if you have any further questions.\n"
      ],
      "metadata": {
        "id": "oyou8ZfayImE"
      }
    }
  ]
}