{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3xOy8TIUcD/iH7uwoFVnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/vince-file/blob/main/Feedforward_Backward%20propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUYwXozivd7M",
        "outputId": "80a18d00-c0ab-4edd-9981-f1eca8615d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight matrix at layer 1: \n",
            "[[0.56800597 0.97613794]\n",
            " [0.46507298 0.86289871]]\n",
            "Weight matrix at layer 2: \n",
            "[[0.72113144 0.39118098]\n",
            " [0.20292534 0.86608318]]\n",
            "The inputs are: [0.54153222 0.3820902 ]\n",
            "The outputs are: [0.56549693 0.59191404]\n",
            "The targets are: [0.58892048 0.79621099]\n",
            "Updated weights after backward propagation:\n",
            "Weight matrix at layer 1: \n",
            "[[0.57157391 0.97970589]\n",
            " [0.46864093 0.86646665]]\n",
            "Weight matrix at layer 2: \n",
            "[[0.73768846 0.407738  ]\n",
            " [0.21948236 0.8826402 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, learn_rate=0.5, input_nodes=2, hidden_nodes=2, output_nodes=2):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "        self.learn_rate = learn_rate\n",
        "\n",
        "        # Defining the layers and attributing random weights for each layer's nodes\n",
        "        layers = [self.input_nodes] + [self.hidden_nodes] + [self.output_nodes]\n",
        "        self.weight = []\n",
        "        for i in range(len(layers) - 1):\n",
        "            w = np.random.rand(layers[i], layers[i+1])\n",
        "            self.weight.append(w)\n",
        "\n",
        "    def __sigmoid(self, hidden_input):\n",
        "        return 1 / (1 + np.exp(-1 * self.learn_rate * hidden_input))\n",
        "\n",
        "    def forward_propagation(self, input):\n",
        "        self.activations = [input]\n",
        "        X_input = input\n",
        "        for i, w in enumerate(self.weight):\n",
        "            hidden_input = np.dot(X_input, w)\n",
        "            activated_hidden_input = self.__sigmoid(hidden_input)\n",
        "            self.activations.append(activated_hidden_input)\n",
        "            X_input = activated_hidden_input\n",
        "            print(\"Weight matrix at layer {}: \\n{}\".format(i+1, w))\n",
        "        return activated_hidden_input\n",
        "\n",
        "    def backward_propagation(self, targets):\n",
        "        deltas = [None] * len(self.weight)\n",
        "        error = targets - self.activations[-1]\n",
        "        delta = error * self.activations[-1] * (1 - self.activations[-1])\n",
        "        deltas[-1] = delta\n",
        "        for i in reversed(range(len(deltas)-1)):\n",
        "            error = np.dot(deltas[i+1], self.weight[i+1].T)\n",
        "            delta = error * self.activations[i+1] * (1 - self.activations[i+1])\n",
        "            deltas[i] = delta\n",
        "        for i in range(len(self.weight)):\n",
        "            gradient = np.dot(self.activations[i].T, deltas[i])\n",
        "            self.weight[i] += self.learn_rate * gradient\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mlp = NN()\n",
        "    input = np.random.rand(mlp.input_nodes)\n",
        "    output = mlp.forward_propagation(input)\n",
        "    targets = np.random.rand(mlp.output_nodes)\n",
        "\n",
        "    print(\"The inputs are: {}\".format(input))\n",
        "    print(\"The outputs are: {}\".format(output))\n",
        "    print(\"The targets are: {}\".format(targets))\n",
        "\n",
        "    mlp.backward_propagation(targets)\n",
        "    print(\"Updated weights after backward propagation:\")\n",
        "    for i, w in enumerate(mlp.weight):\n",
        "        print(\"Weight matrix at layer {}: \\n{}\".format(i+1, w))"
      ]
    }
  ]
}