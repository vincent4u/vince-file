{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsd/aAHi0DLfZx6U5CxCdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/vince-file/blob/main/backward_propagation%20example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKe5vE-3Ckhk",
        "outputId": "f96559ca-2049-4ec9-ee4e-7878f6bd2208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The activated_hidden_output1 are: [0.50969878 0.50886157]\n",
            "The activated_hidden_output1 are: [0.50969878 0.50886157]\n",
            "The inputs are: [0.25 0.02]\n",
            "The outputs are: [0.59495099 0.56707392]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, learn_rate=0.5, input_nodes=2, hidden_nodes=2, output_nodes=2):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "        self.learn_rate = learn_rate\n",
        "\n",
        "        self.weight1 = np.array([[0.30, 0.25], [0.13, 0.42]])\n",
        "        self.weight2 = np.array([[0.67, 0.54], [0.84, 0.52]])\n",
        "\n",
        "    def __sigmoid(self, hidden_input):\n",
        "        return 1 / (1 + np.exp(-1 * self.learn_rate * hidden_input))\n",
        "\n",
        "    def forward_propagation(self, input):\n",
        "        X_input = input\n",
        "        for i in self.weight1:\n",
        "            hidden_input1 = np.dot(X_input, self.weight1)\n",
        "            activated_hidden_output1 = self.__sigmoid(hidden_input1)\n",
        "\n",
        "            hidden_input2 = np.dot(activated_hidden_output1, self.weight2)\n",
        "            activated_hidden_output2 = self.__sigmoid(hidden_input2)\n",
        "\n",
        "        return activated_hidden_output2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input = np.array([0.25, 0.02])\n",
        "    mlp = NN()\n",
        "    output = mlp.forward_propagation(input)\n",
        "\n",
        "    print(\"The inputs are: {}\".format(input))\n",
        "    print(\"The outputs are: {}\".format(output))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, learn_rate=0.5, input_nodes=2, hidden_nodes=2, output_nodes=2):\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "        self.learn_rate = learn_rate\n",
        "\n",
        "        self.weight1 = np.array([[0.30, 0.25], [0.13, 0.42]])\n",
        "        self.weight2 = np.array([[0.67, 0.54], [0.84, 0.52]])\n",
        "\n",
        "        self.activated_hidden_output1 = None\n",
        "        self.activated_hidden_output2 = None\n",
        "\n",
        "    def __sigmoid(self, hidden_input):\n",
        "        return 1 / (1 + np.exp(-1 * self.learn_rate * hidden_input))\n",
        "\n",
        "    def __sigmoid_derivative(self, x):\n",
        "        #return self.learn_rate * x * (1 - x)\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward_propagation(self, input):\n",
        "        X_input = input\n",
        "        hidden_input1 = np.dot(X_input, self.weight1)\n",
        "        self.activated_hidden_output1 = self.__sigmoid(hidden_input1)\n",
        "\n",
        "        hidden_input2 = np.dot(self.activated_hidden_output1, self.weight2)\n",
        "        self.activated_hidden_output2 = self.__sigmoid(hidden_input2)\n",
        "\n",
        "        return self.activated_hidden_output2\n",
        "\n",
        "    def backpropagation(self, input, target):\n",
        "        X_input = input\n",
        "        predicted_output = self.activated_hidden_output2\n",
        "        errors=[]\n",
        "        rmses=[]\n",
        "\n",
        "        error_output = target -  predicted_output\n",
        "        errors.append(error_output) #correct\n",
        "\n",
        "\n",
        "        delta_y = error_output * self.__sigmoid_derivative(predicted_output)    #correct\n",
        "\n",
        "\n",
        "        error_hidden2 = delta_y.dot(self.weight2.T)\n",
        "        print('this is the self weight2', error_hidden2)\n",
        "        delta_hidden2 = error_hidden2 * self.__sigmoid_derivative(self.activated_hidden_output2)\n",
        "\n",
        "        error_hidden1 = delta_hidden2.dot(self.weight1.T)\n",
        "        print(error_hidden1)\n",
        "        delta_hidden1 = error_hidden1 * self.__sigmoid_derivative(self.activated_hidden_output1)\n",
        "\n",
        "        weight2_adjustment = self.learn_rate*np.outer(self.activated_hidden_output1, delta_y)\n",
        "        weight1_adjustment = self.learn_rate*np.outer(X_input, delta_hidden1)\n",
        "\n",
        "        self.weight2 += weight2_adjustment\n",
        "        self.weight1 += weight1_adjustment\n",
        "        rmse = np.sqrt(np.mean(np.square(errors)))\n",
        "        rmses.append(rmse)\n",
        "        print(rmses)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input = np.array([0.25, 0.02])\n",
        "    target = np.array([0.5, 0.05])\n",
        "\n",
        "    mlp = NN()\n",
        "    output = mlp.forward_propagation(input)\n",
        "\n",
        "    print(\"The inputs are: {}\".format(input))\n",
        "    print(\"The outputs are: {}\".format(output))\n",
        "\n",
        "    mlp.backpropagation(input, target)\n",
        "    output_after_bp = mlp.forward_propagation(input)\n",
        "    print(\"The outputs after backpropagation are: {}\".format(output_after_bp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUfZUVtDQc8E",
        "outputId": "80078ea8-dcdd-4826-9a28-f14e88a1e4e6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inputs are: [0.25 0.02]\n",
            "The outputs are: [0.59495099 0.56707392]\n",
            "this is the self weight2 [-0.08387953 -0.08523058]\n",
            "[-0.01129515 -0.01141594]\n",
            "[0.37173991712481386]\n",
            "The outputs after backpropagation are: [0.59423368 0.56302667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight2 = np.array([[0.67, 0.54], [0.84, 0.52]])\n",
        "deltas=np.array([[-0.0228817]  [-0.12694221]])\n",
        "p=deltas.dot(weight2.T)\n",
        "print(p)"
      ],
      "metadata": {
        "id": "XDXsXnWVdast"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}